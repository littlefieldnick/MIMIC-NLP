{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai import *\n",
    "from fastai.text import *\n",
    "\n",
    "from clinical_note_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_roc = []\n",
    "test_roc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/littlefield/MIMIC-NLP/readmission-prediction/data/\"\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = load_data(path, 'clas_export-class.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(clinical_data, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, AUROC(), Precision(), Recall()]).to_fp16()\n",
    "learn.load_encoder('clinical_lm-class-step2_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"c_learner-1-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.get_preds(ds_type=DatasetType.Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(inp, preds, ds=\"Validation\", thresh=None):\n",
    "    best_thresh = thresh\n",
    "    if thresh is None:\n",
    "        print(\"Theshold is not provided, calculating...\")\n",
    "        best_thresh = J_statistic(inp, preds)\n",
    "    final_preds = np.array([1 if p > best_thresh else 0 for p in preds])\n",
    "    acc = pos_accuracy(learn.data.valid_ds.y.items, final_preds, thresh=best_thresh)\n",
    "    auc_score = auc(inp, preds)\n",
    "    f1, precision, recall = f1_precision_recall(inp, final_preds)\n",
    "    \n",
    "    print(\"================================\", ds, \"Metrics for Postive Class ================================\")\n",
    "    print(\"Best Threshold:\", best_thresh)\n",
    "    print(\"Positive Class Acc.:\", acc)\n",
    "    print(\"AUC:\", auc_score)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    \n",
    "    scores = {\"best_thresh\": best_thresh,\n",
    "              \"acc\": acc,\n",
    "              \"auc\": auc_score,\n",
    "              \"f1\": f1, \n",
    "              \"precision\": precision,\n",
    "              \"recall\": recall}\n",
    "    \n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_metrics_1 = eval_model(learn.data.valid_ds.y.items, preds[0][:, 1])\n",
    "valid_roc.append(valid_metrics_1[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (TextList.from_folder(path).filter_by_folder(\"test\"))\n",
    "learn.data.add_test(test)\n",
    "t_preds = learn.get_preds(ds_type=DatasetType.Test)\n",
    "\n",
    "# Create test labels based on order fast.ai read in test files\n",
    "test_labels = []\n",
    "\n",
    "for n in test.items:\n",
    "    if \"pos\" in str(n):\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics_1 = eval_model(test_labels, t_preds[0][:, 1], ds=\"Test\", thresh=valid_metrics_1[\"best_thresh\"])\n",
    "test_roc.append(test_metrics_1[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"c_learner-2-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "valid_metrics_2 = eval_model(learn.data.valid_ds.y.items, preds[0][:, 1])\n",
    "valid_roc.append(valid_metrics_2[\"auc\"])\n",
    "\n",
    "test_metrics_2 = eval_model(test_labels, t_preds[0][:, 1], ds=\"Test\", thresh=valid_metrics_2[\"best_thresh\"])\n",
    "test_roc.append(test_metrics_2[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"c_learner-3-4\")\n",
    "\n",
    "preds = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "valid_metrics_3 = eval_model(learn.data.valid_ds.y.items, preds[0][:, 1])\n",
    "valid_roc.append(valid_metrics_3[\"auc\"])\n",
    "\n",
    "test_metrics_3 = eval_model(test_labels, t_preds[0][:, 1], ds=\"Test\", thresh=valid_metrics_3[\"best_thresh\"])\n",
    "test_roc.append(test_metrics_3[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(\"c_learner-unfreeze-4\")\n",
    "\n",
    "preds = learn.get_preds(ds_type=DatasetType.Valid)\n",
    "valid_metrics_4 = eval_model(learn.data.valid_ds.y.items, preds[0][:, 1])\n",
    "valid_roc.append(valid_metrics_4[\"auc\"])\n",
    "\n",
    "test_metrics_4 = eval_model(test_labels, t_preds[0][:, 1], ds=\"Test\", thresh=valid_metrics_4[\"best_thresh\"])\n",
    "test_roc.append(test_metrics_4[\"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = np.argmax(valid_roc)\n",
    "print(valid_roc[best_auc])\n",
    "print(test_roc[best_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "final_preds = [1 if p > valid_metrics_4[\"best_thresh\"] else 0 for p in t_preds[0][:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4078/(4014+2326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "297/450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
