{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_data_path = \"/home/littlefield/mimic-data/mimiciii/1.4/\"\n",
    "def get_mimic_dataset(table_name):\n",
    "    try:\n",
    "        file = table_name + \".csv\"\n",
    "        return pd.read_csv(mimic_data_path + file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Unable to load data table\", table_name, \"from\", mimic_data_path + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = get_mimic_dataset(\"NOTEEVENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions = get_mimic_dataset(\"ADMISSIONS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions.ADMITTIME = pd.to_datetime(admissions.ADMITTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DISCHTIME = pd.to_datetime(admissions.DISCHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')\n",
    "admissions.DEATHTIME = pd.to_datetime(admissions.DEATHTIME, format = '%Y-%m-%d %H:%M:%S', errors = 'coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Next Unplanned Admission Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by subject_ID and admission date\n",
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "admissions = admissions.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the next admission date and type for each subject using groupby\n",
    "# you have to use groupby otherwise the dates will be from different subjects\n",
    "admissions['NEXT_ADMITTIME'] = admissions.groupby('SUBJECT_ID').ADMITTIME.shift(-1)\n",
    "\n",
    "# get the next admission type\n",
    "admissions['NEXT_ADMISSION_TYPE'] = admissions.groupby('SUBJECT_ID').ADMISSION_TYPE.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Out Elective Admissions and Back Fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows where next admission is elective and replace with naT or nan\n",
    "rows = admissions.NEXT_ADMISSION_TYPE == 'ELECTIVE'\n",
    "admissions.loc[rows,'NEXT_ADMITTIME'] = pd.NaT\n",
    "admissions.loc[rows,'NEXT_ADMISSION_TYPE'] = np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by subject_ID and admission date\n",
    "# it is safer to sort right before the fill in case something changed the order above\n",
    "admissions = admissions.sort_values(['SUBJECT_ID','ADMITTIME'])\n",
    "# back fill (this will take a little while)\n",
    "admissions[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']] = admissions.groupby(['SUBJECT_ID'])[['NEXT_ADMITTIME','NEXT_ADMISSION_TYPE']].fillna(method = 'bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Number of Days till Next Admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admissions['DAYS_NEXT_ADMIT'] =  (admissions.NEXT_ADMITTIME - admissions.DISCHTIME).dt.total_seconds()/(24*60*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter to Use Discharge Notes Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dis = notes.loc[notes.CATEGORY == 'Discharge summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_dis_last = (notes_dis.groupby(['SUBJECT_ID','HADM_ID']).nth(-1)).reset_index()\n",
    "assert notes_dis_last.duplicated(['HADM_ID']).sum() == 0, 'Multiple discharge summaries per admission'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge ADMISSIONS and NOTEEVENTS Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes = pd.merge(admissions[['SUBJECT_ID','HADM_ID','ADMITTIME','DISCHTIME','DAYS_NEXT_ADMIT','NEXT_ADMITTIME','ADMISSION_TYPE','DEATHTIME']],\n",
    "                        notes_dis_last[['SUBJECT_ID','HADM_ID','TEXT']], \n",
    "                        on = ['SUBJECT_ID','HADM_ID'],\n",
    "                        how = 'left')\n",
    "assert len(admissions) == len(adm_notes), 'Number of rows increased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Amount of Admissions Missing Discharge Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes.TEXT.isnull().sum() / len(adm_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes.groupby('ADMISSION_TYPE').apply(lambda g: g.TEXT.isnull().sum())/adm_notes.groupby('ADMISSION_TYPE').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes_clean = adm_notes.loc[adm_notes.ADMISSION_TYPE != 'NEWBORN'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Output Label: Patients who are readmitted within 30 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_notes_clean['OUTPUT_LABEL'] = (adm_notes_clean.DAYS_NEXT_ADMIT < 30).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of positive samples:', (adm_notes_clean.OUTPUT_LABEL == 1).sum())\n",
    "print('Number of negative samples:',  (adm_notes_clean.OUTPUT_LABEL == 0).sum())\n",
    "print('Total samples:', len(adm_notes_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Training/Validation/Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the samples\n",
    "adm_notes_clean = adm_notes_clean.sample(n = len(adm_notes_clean), random_state = 42)\n",
    "adm_notes_clean = adm_notes_clean.reset_index(drop = True)\n",
    "\n",
    "# Save 30% of the data as validation and test data \n",
    "valid_test=adm_notes_clean.sample(frac=0.30,random_state=42)\n",
    "\n",
    "test = valid_test.sample(frac = 0.5, random_state = 42)\n",
    "valid = valid_test.drop(test.index)\n",
    "\n",
    "# use the rest of the data as training data\n",
    "train = adm_notes_clean.drop(valid_test.index)\n",
    "\n",
    "print('Test prevalence(n = %d):'%len(test), test.OUTPUT_LABEL.sum()/ len(test))\n",
    "print('Valid prevalence(n = %d):'%len(valid), valid.OUTPUT_LABEL.sum()/ len(valid))\n",
    "print('Train all prevalence(n = %d):'%len(train), train.OUTPUT_LABEL.sum()/ len(train))\n",
    "print('all samples (n = %d)'%len(adm_notes_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prevalence is low, subsample negatives in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training data into positive and negative\n",
    "rows_pos = train.OUTPUT_LABEL == 1\n",
    "train_pos = train.loc[rows_pos]\n",
    "train_neg = train.loc[~rows_pos]\n",
    "\n",
    "# merge the balanced data\n",
    "train_sub = pd.concat([train_pos, train_neg.sample(n = len(train_pos), random_state = 42)],axis = 0)\n",
    "\n",
    "# shuffle the order of training samples \n",
    "train_sub = train_sub.sample(n = len(train_sub), random_state = 42).reset_index(drop = True)\n",
    "\n",
    "print('Train prevalence (n = %d):'%len(train_sub), train_sub.OUTPUT_LABEL.sum()/ len(train_sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Notes: Remove new lines and carriage returns, and replace NaNs with '   '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df):\n",
    "    df.TEXT = df.TEXT.fillna(' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\n',' ')\n",
    "    df.TEXT = df.TEXT.str.replace('\\r',' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess_text(train)\n",
    "train_sub = preprocess_text(train_sub)\n",
    "valid = preprocess_text(valid)\n",
    "test = preprocess_text(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save training, valid, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = \"/home/littlefield/MIMIC-NLP/readmission-prediction/data\"\n",
    "if not os.path.exists(data_pth):\n",
    "    os.mkdir(data_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(data_pth + \"/train_complete.csv\")\n",
    "train_sub.to_csv(data_pth + \"/train_subsample.csv\")\n",
    "valid.to_csv(data_pth + \"/valid.csv\")\n",
    "test.to_csv(data_pth + \"/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub[\"is_valid\"] = False\n",
    "valid[\"is_valid\"] = True\n",
    "\n",
    "fast_form = pd.merge(train_sub, valid, how=\"outer\")\n",
    "fast_form.to_csv(data_pth + \"/train_valid_fastai.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
