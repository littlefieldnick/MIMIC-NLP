{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import sys\n",
    "import spacy\n",
    "import re\n",
    "import stanfordnlp\n",
    "import time\n",
    "import scispacy\n",
    "from tqdm import tqdm\n",
    "from heuristic_tokenize import sent_tokenize_rules \n",
    "\n",
    "\n",
    "# update these constants to run this script\n",
    "OUTPUT_DIR = '/home/littlefield/MIMIC-NLP/readmission-prediction/data/' #this path will contain tokenized notes. This dir will be the input dir for create_pretrain_data.sh\n",
    "MIMIC_NOTES_FILE = '/home/littlefield/mimic-data/mimiciii/1.4/NOTEEVENTS.csv' #this is the path to mimic data if you're reading from a csv. Else uncomment the code to read from database below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting sentence boundaries\n",
    "def sbd_component(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # define sentence start if period + titlecase token\n",
    "        if token.text == '.' and doc[i+1].is_title:\n",
    "            doc[i+1].sent_start = True\n",
    "        if token.text == '-' and doc[i+1].text != '-':\n",
    "            doc[i+1].sent_start = True\n",
    "    return doc\n",
    "\n",
    "#convert de-identification text into one token\n",
    "def fix_deid_tokens(text, processed_text):\n",
    "    deid_regex  = r\"\\[\\*\\*.{0,15}.*?\\*\\*\\]\" \n",
    "    if text:\n",
    "        indexes = [m.span() for m in re.finditer(deid_regex,text,flags=re.IGNORECASE)]\n",
    "    else:\n",
    "        indexes = []\n",
    "    for start,end in indexes:\n",
    "        processed_text.merge(start_idx=start,end_idx=end)\n",
    "    return processed_text\n",
    "\n",
    "def process_section(section, note, processed_sections):\n",
    "    # perform spacy processing on section\n",
    "    processed_section = nlp(section['sections'])\n",
    "    processed_section = fix_deid_tokens(section['sections'], processed_section)\n",
    "    processed_sections.append(processed_section)\n",
    "\n",
    "def process_note_helper(note):\n",
    "    # split note into sections\n",
    "    note_sections = sent_tokenize_rules(note)\n",
    "    processed_sections = []\n",
    "    section_frame = pd.DataFrame({'sections':note_sections})\n",
    "    section_frame.apply(process_section, args=(note,processed_sections,), axis=1)\n",
    "    return(processed_sections)\n",
    "\n",
    "def process_text(sent, note):\n",
    "    sent_text = sent['sents'].text\n",
    "    if len(sent_text) > 0 and sent_text.strip() != '\\n':\n",
    "        if '\\n' in sent_text:\n",
    "            sent_text = sent_text.replace('\\n', ' ')\n",
    "        note['TEXT'] += sent_text + '\\n'  \n",
    "\n",
    "def get_sentences(processed_section, note):\n",
    "    # get sentences from spacy processing\n",
    "    sent_frame = pd.DataFrame({'sents': list(processed_section['sections'].sents)})\n",
    "    sent_frame.apply(process_text, args=(note,), axis=1)\n",
    "\n",
    "def process_note(note):\n",
    "    try:\n",
    "        note_text = note['TEXT'] #unicode(note['text'])\n",
    "        note['TEXT'] = ''\n",
    "        processed_sections = process_note_helper(note_text)\n",
    "        ps = {'sections': processed_sections}\n",
    "        ps = pd.DataFrame(ps)\n",
    "        ps.apply(get_sentences, args=(note,), axis=1)\n",
    "        return note \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print ('error', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import * \n",
    "notes = get_mimic_dataset(\"NOTEEVENTS\")\n",
    "adm = get_mimic_dataset(\"ADMISSIONS\")\n",
    "\n",
    "readmission_data = preprocess(adm, notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Discharge summary'\n",
    "\n",
    "start = time.time()\n",
    "tqdm.pandas()\n",
    "\n",
    "print('Begin reading notes')\n",
    "print(\"Filtering on category: \", category)\n",
    "\n",
    "print('Number of notes: %d' %len(readmission_data.index))\n",
    "readmission_data['ind'] = list(range(len(readmission_data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_sci_md', disable=['tagger','ner'])\n",
    "nlp.add_pipe(sbd_component, before='parser')  \n",
    "\n",
    "formatted_notes = readmission_data[:10].progress_apply(process_note, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR  + category + '.txt','w') as f:\n",
    "    for text in formatted_notes['TEXT']:\n",
    "        if text != None and len(text) != 0 :\n",
    "            f.write(text)\n",
    "            f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTPUT_DIR  + category + ' labels.txt','w') as f:\n",
    "    for text, label in zip(formatted_notes[\"TEXT\"], formatted_notes[\"OUTPUT_LABEL\"]):\n",
    "         if text != None and len(text) != 0 :\n",
    "                f.write(str(label))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "end = time.time()\n",
    "print (end-start)\n",
    "print (\"Done formatting notes\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
