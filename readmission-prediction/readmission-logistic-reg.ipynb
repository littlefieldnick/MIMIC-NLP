{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pth = \"/home/littlefield/MIMIC-NLP/readmission-prediction/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data = (TextList.from_csv(data_pth, \"train_valid_fastai.csv\", cols='TEXT').split_from_df(col=10)\n",
    "        .label_from_df(cols=9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of processed tokens:\", len(clinical_data.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_data.train.x[13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map Clinical Notes to Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_terms = len(clinical_data.vocab.itos)\n",
    "n_docs = len(clinical_data.train.x)\n",
    "\n",
    "print(\"There are\", n_terms, \"terms\")\n",
    "print(\"There are\", n_docs, \"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_token_counter = lambda clinical_index: Counter(clinical_data.train.x[clinical_index].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: clinical note index, n_terms, and tokenizer function\n",
    "# output: embedding vector for the review\n",
    "def count_vectorizer(clinical_index, n_terms, make_token_counter):\n",
    "    embedding_vector = np.zeros(n_terms)   \n",
    "    term_toknzr = make_token_counter(clinical_index)\n",
    "    keys = list(term_toknzr.keys())\n",
    "    values = list(term_toknzr.values())\n",
    "    embedding_vector[keys] = values\n",
    "    return embedding_vector\n",
    "\n",
    "embedding_vector = count_vectorizer(0, n_terms, clinical_token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The clinical note is embedded in a {len(embedding_vector)} dimensional vector')\n",
    "embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to build the full document-term matrix\n",
    "print(f'there are {n_docs} reviews, and {n_terms} unique tokens in the vocabulary')\n",
    "def make_full_doc_term_matrix(count_vectorizer,n_terms,n_docs):\n",
    "    \n",
    "    # loop through the movie reviews\n",
    "    for doc_index in range(n_docs):\n",
    "        \n",
    "        # make the embedding vector for the current review\n",
    "        embedding_vector = count_vectorizer(doc_index,n_terms, clinical_token_counter)    \n",
    "            \n",
    "        # append the embedding vector to the document-term matrix\n",
    "        if(doc_index == 0):\n",
    "            A = embedding_vector\n",
    "        else:\n",
    "            A = np.vstack((A,embedding_vector))\n",
    "            \n",
    "    # return the document-term matrix\n",
    "    return A\n",
    "\n",
    "# Build the full document term matrix for the movie_reviews training set\n",
    "A = make_full_doc_term_matrix(count_vectorizer, n_terms, n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Matrix Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNZ = np.count_nonzero(A)\n",
    "sparsity = (A.size-NNZ)/A.size\n",
    "print(f'Only {NNZ} of the {A.size} elements in the document-term matrix are nonzero')\n",
    "print(f'The sparsity of the document-term matrix is {sparsity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the document-term matrix in CSR format\n",
    "# i.e. return (values, column_indices, row_pointer)\n",
    "def get_doc_term_matrix(text_list, n_terms):\n",
    "    \n",
    "    # inputs:\n",
    "    #    text_list, a TextList object\n",
    "    #    n_terms, the number of tokens in our IMDb vocabulary\n",
    "    \n",
    "    # output: \n",
    "    #    the CSR format sparse representation of the document-term matrix in the form of a\n",
    "    #    scipy.sparse.csr.csr_matrix object\n",
    "\n",
    "    \n",
    "    # initialize arrays\n",
    "    values = []\n",
    "    column_indices = []\n",
    "    row_pointer = []\n",
    "    row_pointer.append(0)\n",
    "\n",
    "    # from the TextList object\n",
    "    for _, doc in enumerate(text_list):\n",
    "        feature_counter = Counter(doc.data)\n",
    "        column_indices.extend(feature_counter.keys())\n",
    "        values.extend(feature_counter.values())\n",
    "        # Tack on N (number of nonzero elements in the matrix) to the end of the row_pointer array\n",
    "        row_pointer.append(len(values))\n",
    "        \n",
    "    return scipy.sparse.csr_matrix((values, column_indices, row_pointer),\n",
    "                                   shape=(len(row_pointer) - 1, n_terms),\n",
    "                                   dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = get_doc_term_matrix(clinical_data.train.x, n_terms)\n",
    "train_y = clinical_data.train.y.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = get_doc_term_matrix(clinical_data.valid.x, n_terms)\n",
    "valid_y = clinical_data.valid.y.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C = 0.0001, penalty = 'l2', random_state = 999)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(valid_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Score:\", clf.score(train_x, train_y))\n",
    "print(\"Validation Score:\", clf.score(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "print(precision_recall_fscore_support(valid_y, preds))\n",
    "print(roc_auc_score(valid_y, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
